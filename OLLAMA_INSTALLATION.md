# üöÄ Instalaci√≥n de Llama con Ollama - Paso a Paso

## üìã Requisitos Previos

### Hardware M√≠nimo
```
CPU: 4 cores
RAM: 8 GB
Disco: 10 GB libres
GPU: Opcional (recomendado NVIDIA)
```

### Software
```
Python: 3.9+
pip: Actualizado
Sistema: Windows, Mac, o Linux
```

---

## 1Ô∏è‚É£ Instalar Ollama

### üêß Linux / Mac
```bash
# Un solo comando
curl -fsSL https://ollama.com/install.sh | sh

# Verificar instalaci√≥n
ollama --version
```

### ü™ü Windows
```
1. Descargar de: https://ollama.com/download/windows
2. Ejecutar instalador OllamaSetup.exe
3. Seguir el wizard
4. Verificar en CMD: ollama --version
```

### ‚úÖ Verificaci√≥n
```bash
# Debe mostrar algo como: "ollama version 0.1.xx"
ollama --version

# Debe responder: "Ollama is running"
curl http://localhost:11434
```

---

## 2Ô∏è‚É£ Descargar Modelo Llama

### Opci√≥n A: Llama 3.2 (Recomendado para empezar)

```bash
# Ligero y r√°pido (1 billion params)
ollama pull llama3.2:1b
# Tama√±o: ~1.3 GB
# Tiempo: 2-5 min
# RAM: 4 GB

# Balance (3 billion params) ‚≠ê RECOMENDADO
ollama pull llama3.2:3b
# Tama√±o: ~2 GB
# Tiempo: 5-10 min
# RAM: 6 GB
```

### Opci√≥n B: Llama 3.1 (M√°s preciso)

```bash
# Preciso (8 billion params)
ollama pull llama3.1:8b
# Tama√±o: ~4.7 GB
# Tiempo: 10-20 min
# RAM: 8 GB

# Muy preciso (70 billion params) - Requiere GPU potente
ollama pull llama3.1:70b
# Tama√±o: ~40 GB
# Tiempo: 1-2 horas
# RAM: 32 GB + GPU 24GB
```

### ‚úÖ Verificar Modelos Descargados
```bash
ollama list

# Output ejemplo:
# NAME              SIZE    MODIFIED
# llama3.2:3b       2.0 GB  2 hours ago
# llama3.1:8b       4.7 GB  1 day ago
```

---

## 3Ô∏è‚É£ Probar Ollama

### Test B√°sico
```bash
# Iniciar chat interactivo
ollama run llama3.2:3b

# Ejemplo de conversaci√≥n:
# >>> ¬øCu√°l es la capital de Per√∫?
# La capital de Per√∫ es Lima.
# 
# >>> /bye
```

### Test con API
```bash
# Request simple
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "¬øQu√© es Python?"
}'

# Debe responder con JSON
```

---

## 4Ô∏è‚É£ Instalar Dependencias Python

```bash
# Crear entorno virtual (recomendado)
python -m venv venv

# Activar
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# Instalar LangChain + Ollama
pip install langchain-ollama

# O instalar todas las dependencias
pip install -r requirements.txt
```

---

## 5Ô∏è‚É£ Configurar tu Proyecto

### Actualizar `.env`
```env
# Estrategia por defecto
DEFAULT_AI_STRATEGY=llama

# Modelo Llama
LLAMA_MODEL=llama3.2:3b
LLAMA_BASE_URL=http://localhost:11434
```

### Actualizar `config.py`
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # ... otros campos ...
    
    # Llama
    LLAMA_MODEL: str = "llama3.2:3b"
    LLAMA_BASE_URL: str = "http://localhost:11434"
    
    class Config:
        env_file = ".env"
```

---

## 6Ô∏è‚É£ Probar LlamaClient

### Crear archivo de prueba
```python
# test_llama.py
from backend.app.services.ai.llama_client import llama_client

# Texto de prueba
ocr_text = """
LECHE GLORIA
ENTERA
Contenido neto: 1000 ml
Precio: S/ 5.50
Lote: L20250212
Vence: 15/06/2025
"""

# Extraer
try:
    result = llama_client.extract(ocr_text)
    
    print("‚úÖ Extracci√≥n exitosa:")
    print(f"Nombre: {result['name']}")
    print(f"Marca: {result['brand']}")
    print(f"Tama√±o: {result['size']}")
    print(f"Precio: {result['price']}")
    
except Exception as e:
    print(f"‚ùå Error: {e}")
```

### Ejecutar prueba
```bash
python test_llama.py

# Output esperado:
# ‚úÖ Extracci√≥n exitosa:
# Nombre: Leche Gloria Entera
# Marca: Gloria
# Tama√±o: 1000ml
# Precio: 5.5
```

---

## 7Ô∏è‚É£ Integrar con tu Endpoint

```python
# En tu endpoint /inventory/from-images
from backend.app.services.ai.ai_extractor_service import ai_extractor_service

@router.post("/inventory/from-images")
async def process_product_images(...):
    # ... tu c√≥digo de OCR ...
    
    # Usar Llama
    product_info = await asyncio.to_thread(
        ai_extractor_service.extract_product_info,
        ocr_data,
        strategy="llama"  # üëà Aqu√≠
    )
    
    # ... resto del c√≥digo ...
```

---

## üêõ Soluci√≥n de Problemas

### Error: "Ollama is not running"
```bash
# Soluci√≥n 1: Iniciar Ollama manualmente
ollama serve

# Soluci√≥n 2 (Linux): Habilitar servicio
sudo systemctl enable ollama
sudo systemctl start ollama

# Soluci√≥n 3 (Windows): Reiniciar servicio
# Buscar "Ollama" en Servicios de Windows
```

### Error: "Model not found"
```bash
# Listar modelos
ollama list

# Si no aparece, descargar
ollama pull llama3.2:3b

# Verificar descarga
ollama list
```

### Error: "Timeout" o muy lento
```bash
# Problema: Sin GPU o poca RAM

# Soluci√≥n 1: Usar modelo m√°s peque√±o
ollama pull llama3.2:1b

# Soluci√≥n 2: Aumentar timeout
# En llama_client.py:
# LlamaClient(timeout=120)  # 2 minutos

# Soluci√≥n 3: Cerrar otras apps
# Liberar RAM para Ollama
```

### Error: "Port 11434 already in use"
```bash
# Ver qu√© est√° usando el puerto
# Linux/Mac:
lsof -i :11434
# Windows:
netstat -ano | findstr :11434

# Matar el proceso
# Linux/Mac:
kill -9 [PID]
# Windows:
taskkill /PID [PID] /F

# Reiniciar Ollama
ollama serve
```

### Llama da resultados pobres
```bash
# Soluci√≥n 1: Usar modelo m√°s grande
ollama pull llama3.1:8b

# Soluci√≥n 2: Ajustar temperatura
# En llama_client.py:
# OllamaLLM(temperature=0.1)  # M√°s conservador

# Soluci√≥n 3: Mejorar el prompt
# Editar _create_prompt_template() en llama_client.py
```

---

## üîß Configuraci√≥n Avanzada

### GPU (NVIDIA)
```bash
# Verificar que Ollama detecta GPU
ollama run llama3.2:3b --verbose

# Debe mostrar:
# "Using GPU: NVIDIA GeForce RTX..."

# Si no detecta:
# 1. Instalar drivers NVIDIA
# 2. Instalar CUDA toolkit
# 3. Reiniciar Ollama
```

### M√∫ltiples Modelos
```python
# Puedes tener varios modelos
from backend.app.services.ai.llama_client import LlamaClient

# Cliente r√°pido
fast_client = LlamaClient(model="llama3.2:1b")

# Cliente preciso
accurate_client = LlamaClient(model="llama3.1:8b")

# Usar seg√∫n necesidad
if simple_product:
    result = fast_client.extract(text)
else:
    result = accurate_client.extract(text)
```

### Optimizaci√≥n de Performance
```python
# En llama_client.py
self.llm = OllamaLLM(
    model="llama3.2:3b",
    temperature=0,
    num_predict=512,      # Limitar tokens (m√°s r√°pido)
    num_ctx=2048,         # Contexto m√°s corto
    top_k=10,             # Top-k sampling
    top_p=0.9,            # Nucleus sampling
    repeat_penalty=1.1    # Evitar repetici√≥n
)
```

---

## üìä Benchmark de Modelos

### En tu sistema (prueba cada uno):

```bash
# Test de velocidad
time ollama run llama3.2:1b "Extrae: LECHE GLORIA 1L"
time ollama run llama3.2:3b "Extrae: LECHE GLORIA 1L"
time ollama run llama3.1:8b "Extrae: LECHE GLORIA 1L"
```

**Resultados t√≠picos (sin GPU):**
```
llama3.2:1b  ‚Üí 5-10s  (75% precisi√≥n)
llama3.2:3b  ‚Üí 10-15s (80% precisi√≥n)
llama3.1:8b  ‚Üí 20-30s (85% precisi√≥n)
```

**Resultados t√≠picos (con GPU RTX 3060):**
```
llama3.2:1b  ‚Üí 2-3s   (75% precisi√≥n)
llama3.2:3b  ‚Üí 3-5s   (80% precisi√≥n)
llama3.1:8b  ‚Üí 8-12s  (85% precisi√≥n)
```

---

## üéØ Recomendaciones por Caso

### Desarrollo Local
```bash
ollama pull llama3.2:1b
# R√°pido para pruebas
```

### Producci√≥n (Servidor con GPU)
```bash
ollama pull llama3.1:8b
# Mejor balance calidad/velocidad
```

### M√°xima Precisi√≥n (Servidor potente)
```bash
ollama pull llama3.1:70b
# Requiere: 32GB RAM + GPU 24GB
```

### Sin GPU
```bash
ollama pull llama3.2:1b
# √önico viable sin GPU
# O mejor: usa Gemini (gratis, en la nube)
```

---

## üìö Recursos Adicionales

### Documentaci√≥n
- **Ollama**: https://ollama.com/docs
- **LangChain**: https://python.langchain.com/docs
- **Llama**: https://llama.meta.com

### Comunidad
- **Discord Ollama**: https://discord.gg/ollama
- **GitHub Issues**: https://github.com/ollama/ollama/issues

### Modelos Alternativos
```bash
# Mistral (alternativa a Llama)
ollama pull mistral:7b

# CodeLlama (especializado en c√≥digo)
ollama pull codellama:7b

# Phi-2 (peque√±o pero bueno)
ollama pull phi:2.7b
```

---

## ‚úÖ Checklist Final

- [ ] 1. Ollama instalado (`ollama --version`)
- [ ] 2. Modelo descargado (`ollama list`)
- [ ] 3. Ollama corriendo (`curl http://localhost:11434`)
- [ ] 4. LangChain instalado (`pip install langchain-ollama`)
- [ ] 5. LlamaClient probado (`python test_llama.py`)
- [ ] 6. Integrado en endpoint (`strategy="llama"`)
- [ ] 7. Logs verificados (sin errores)

---

## üéâ ¬°Listo!

Tu sistema ahora tiene:
- ‚úÖ Ollama corriendo localmente
- ‚úÖ Llama 3.2 disponible
- ‚úÖ LlamaClient funcionando
- ‚úÖ Integrado con tu sistema

**Pr√≥ximo paso:**
Prueba tu endpoint con fotos reales y compara:
- Gemini (nube, gratis, r√°pido)
- OpenAI (nube, pago, preciso)
- Llama (local, gratis, privado)

Elige el que mejor se adapte a tus necesidades! üöÄ

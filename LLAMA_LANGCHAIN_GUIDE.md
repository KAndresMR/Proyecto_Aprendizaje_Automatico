# ğŸ¦™ GuÃ­a Completa: Llama + LangChain

## ğŸ“š Â¿QuÃ© es Llama?

### Concepto
**Llama** es un modelo de lenguaje grande (LLM) de **cÃ³digo abierto** creado por Meta (Facebook).

### CaracterÃ­sticas:
- ğŸ†“ **Gratis**: 100% open-source
- ğŸ  **Local**: Corre en TU computadora (no envÃ­a datos a internet)
- âš¡ **RÃ¡pido**: Si tienes buena GPU
- ğŸ”’ **Privado**: Los datos NUNCA salen de tu servidor

### Versiones Disponibles:
```
Llama 3.2:
â”œâ”€â”€ llama3.2:1b     â†’ 1 billion params (muy rÃ¡pido, poca precisiÃ³n)
â”œâ”€â”€ llama3.2:3b     â†’ 3 billion params (balance)
â””â”€â”€ llama3.2:11b    â†’ 11 billion params (lento, muy preciso)

Llama 3.1:
â”œâ”€â”€ llama3.1:8b     â†’ 8 billion params (recomendado)
â”œâ”€â”€ llama3.1:70b    â†’ 70 billion params (requiere GPU potente)
â””â”€â”€ llama3.1:405b   â†’ 405 billion params (requiere cluster)
```

### Â¿CÃ³mo funciona Llama?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLAMA (Modelo de IA)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  ENTRADA:  "Texto OCR: Leche Gloria 1L..."         â”‚
â”‚            â†“                                        â”‚
â”‚  PROCESO:  [Neurona Layer 1] â†’ [Layer 2] â†’ ...     â”‚
â”‚            Analiza patterns, extrae entidades       â”‚
â”‚            â†“                                        â”‚
â”‚  SALIDA:   "{ "name": "Leche Gloria", ... }"       â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ventajas vs Desventajas

| âœ… VENTAJAS | âŒ DESVENTAJAS |
|-------------|----------------|
| Gratis 100% | Requiere GPU/CPU potente |
| Privacidad total | InstalaciÃ³n compleja |
| Sin lÃ­mites de API | Lento sin GPU |
| Sin internet | Consume RAM (4-16 GB) |
| Personalizable | Menos preciso que GPT-4 |

---

## ğŸ”— Â¿QuÃ© es LangChain?

### Concepto
**LangChain** es un **framework** (conjunto de herramientas) para construir aplicaciones con LLMs.

### AnalogÃ­a:
```
Si Llama es un "chef" (IA que cocina respuestas)
â†’ LangChain es la "cocina profesional" (herramientas para trabajar mejor)
```

### Â¿QuÃ© hace LangChain?

```python
# âŒ SIN LangChain (complicado)
import requests
response = requests.post("http://localhost:11434/api/generate", {
    "model": "llama3.2",
    "prompt": f"Extrae datos de: {texto}",
    "stream": False
})
data = response.json()['response']
# Necesitas parsear manualmente, manejar errores, etc.

# âœ… CON LangChain (fÃ¡cil)
from langchain_ollama import OllamaLLM
llm = OllamaLLM(model="llama3.2")
response = llm.invoke("Extrae datos de: " + texto)
# Â¡Listo! LangChain maneja todo
```

### Componentes de LangChain

1. **LLMs**: ConexiÃ³n a modelos (Llama, OpenAI, etc.)
2. **Prompts**: Templates para instrucciones
3. **Chains**: Secuencias de pasos (Prompt â†’ LLM â†’ Parser)
4. **Memory**: Recordar conversaciones anteriores
5. **Agents**: IA que decide quÃ© hacer

---

## ğŸ”„ Flujo Completo en Tu Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USUARIO TOMA FOTOS                                           â”‚
â”‚    ğŸ“¸ Frontal | ğŸ“¸ Izquierda | ğŸ“¸ Derecha                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. OCR EXTRAE TEXTO (Pytesseract)                              â”‚
â”‚    Input:  ImÃ¡genes (bytes)                                     â”‚
â”‚    Proceso: Tesseract analiza pixeles â†’ texto                   â”‚
â”‚    Output: "LECHE GLORIA\n1000 ml\n7750182001564"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. AI EXTRACTOR (Tu cÃ³digo actual)                             â”‚
â”‚    Elige estrategia: Gemini | OpenAI | Llama | Mock            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. LLAMA CLIENT (LangChain)                                    â”‚
â”‚                                                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚ A. OLLAMA (Servidor Local)               â”‚                â”‚
â”‚    â”‚    - Corre en http://localhost:11434     â”‚                â”‚
â”‚    â”‚    - Aloja el modelo Llama 3.2           â”‚                â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚               â†“                                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚ B. LANGCHAIN (Framework)                 â”‚                â”‚
â”‚    â”‚    - OllamaLLM: ConexiÃ³n a Ollama        â”‚                â”‚
â”‚    â”‚    - PromptTemplate: Estructura prompt   â”‚                â”‚
â”‚    â”‚    - Chain: Prompt â†’ LLM â†’ Parser        â”‚                â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚               â†“                                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚ C. LLAMA MODEL (IA)                      â”‚                â”‚
â”‚    â”‚    Input:  Prompt + OCR text             â”‚                â”‚
â”‚    â”‚    Proceso: Neural network processing    â”‚                â”‚
â”‚    â”‚    Output: JSON string                   â”‚                â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚               â†“                                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚ D. JSON PARSER (Tu cÃ³digo)               â”‚                â”‚
â”‚    â”‚    - Limpia markdown (```)               â”‚                â”‚
â”‚    â”‚    - Extrae JSON con regex               â”‚                â”‚
â”‚    â”‚    - Parsea con json.loads()             â”‚                â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. RESULTADO FINAL                                              â”‚
â”‚    {                                                            â”‚
â”‚      "name": "Leche Gloria Entera",                            â”‚
â”‚      "brand": "Gloria",                                         â”‚
â”‚      "size": "1000ml",                                          â”‚
â”‚      ...                                                        â”‚
â”‚    }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Desglose TÃ©cnico

### 1. OLLAMA (Servidor)

**Â¿QuÃ© hace?**
- Aloja modelos LLM localmente
- API REST para comunicaciÃ³n
- Maneja memoria GPU/CPU

**InstalaciÃ³n:**
```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Descargar de: https://ollama.com/download

# Verificar instalaciÃ³n
ollama --version
```

**Descargar modelo:**
```bash
# Llama 3.2 (1B - rÃ¡pido)
ollama pull llama3.2:1b

# Llama 3.2 (3B - recomendado)
ollama pull llama3.2:3b

# Llama 3.1 (8B - mÃ¡s preciso)
ollama pull llama3.1:8b
```

**Servidor corre en:**
```
http://localhost:11434
```

**Ejemplo de request directo:**
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Â¿QuÃ© es Python?"
}'
```

---

### 2. LANGCHAIN (Framework)

**Componentes que usas:**

#### A. OllamaLLM
```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(
    model="llama3.2",      # Modelo a usar
    temperature=0,         # 0 = determinista, 1 = creativo
    base_url="http://localhost:11434"  # Servidor Ollama
)

# Entrada: String (prompt)
# Salida: String (respuesta del modelo)
response = llm.invoke("Â¿CuÃ¡l es la capital de PerÃº?")
# â†’ "La capital de PerÃº es Lima."
```

#### B. PromptTemplate
```python
from langchain_core.prompts import PromptTemplate

template = PromptTemplate(
    input_variables=["producto", "precio"],
    template="""
    Producto: {producto}
    Precio: {precio}
    
    Â¿Es caro o barato?
    """
)

# Entrada: Dict con variables
# Salida: Prompt formateado
prompt = template.format(producto="iPhone", precio="$1000")
# â†’ "Producto: iPhone\nPrecio: $1000\nÂ¿Es caro o barato?"
```

#### C. Chain (Cadena)
```python
from langchain_core.prompts import PromptTemplate
from langchain_ollama import OllamaLLM

template = PromptTemplate(...)
llm = OllamaLLM(...)

# Crear cadena (prompt â†’ llm)
chain = template | llm

# Entrada: Dict
# Proceso: Template formatea â†’ LLM procesa
# Salida: String
response = chain.invoke({"producto": "iPhone", "precio": "$1000"})
```

---

### 3. TU CÃ“DIGO ACTUAL (LlamaClient)

```python
class LlamaClient:
    def __init__(self):
        # PASO 1: Conectar a Ollama
        self.llm = OllamaLLM(model="llama3.2", temperature=0)
        
        # PASO 2: Crear template del prompt
        self.prompt = PromptTemplate(
            input_variables=["ocr_text"],
            template="Extrae datos de: {ocr_text}"
        )
    
    def extract(self, ocr_text: str) -> dict:
        # PASO 3: Crear chain (template + llm)
        chain = self.prompt | self.llm
        
        # PASO 4: Ejecutar
        # Input: {"ocr_text": "LECHE GLORIA 1L..."}
        # Output: "```json\n{...}\n```"
        response = chain.invoke({"ocr_text": ocr_text})
        
        # PASO 5: Limpiar y parsear JSON
        return self._extract_json_from_response(response)
```

---

## ğŸ“Š ComparaciÃ³n: Llama vs Gemini vs OpenAI

| Aspecto | Llama 3.2 | Gemini | OpenAI |
|---------|-----------|---------|---------|
| **UbicaciÃ³n** | Tu servidor | Nube Google | Nube OpenAI |
| **Latencia** | 2-10s | 2-3s | 3-5s |
| **Privacidad** | â­â­â­â­â­ | â­â­ | â­â­ |
| **Costo** | $0 (electricidad) | $0 (gratis) | $0.001/req |
| **PrecisiÃ³n** | â­â­â­ 75% | â­â­â­â­ 85% | â­â­â­â­â­ 92% |
| **Setup** | âš ï¸ Complejo | âœ… FÃ¡cil | âœ… FÃ¡cil |
| **RAM necesaria** | 4-16 GB | 0 | 0 |
| **GPU necesaria** | Recomendada | No | No |

---

## ğŸ¯ Â¿CuÃ¡ndo usar Llama?

### âœ… USA LLAMA cuando:
- Manejas datos sensibles (privacidad crÃ­tica)
- Tienes buena GPU (RTX 3060+ o similar)
- Quieres evitar costos de API
- Tienes volumen MUY alto (>100k requests/dÃ­a)
- No tienes internet confiable

### âŒ NO USES LLAMA cuando:
- No tienes GPU (serÃ¡ MUY lento)
- Necesitas mÃ¡xima precisiÃ³n
- Quieres desarrollo rÃ¡pido (usa Gemini)
- Tienes presupuesto para APIs

---

## ğŸ’» Requisitos de Hardware

### MÃ­nimo (Llama 3.2:1b)
```
CPU: 4 cores
RAM: 4 GB
GPU: Ninguna (lento)
Velocidad: ~15s por request
```

### Recomendado (Llama 3.2:3b)
```
CPU: 6 cores
RAM: 8 GB
GPU: NVIDIA GTX 1660 (6 GB VRAM)
Velocidad: ~3s por request
```

### Ã“ptimo (Llama 3.1:8b)
```
CPU: 8 cores
RAM: 16 GB
GPU: NVIDIA RTX 3060 (12 GB VRAM)
Velocidad: ~2s por request
```

---

## ğŸ”§ PrÃ³ximos pasos

Te voy a crear una versiÃ³n mejorada de tu `LlamaClient` con:
- âœ… Mejor manejo de errores
- âœ… ValidaciÃ³n de Ollama
- âœ… Logs detallados
- âœ… MÃºltiples modelos
- âœ… Retry automÃ¡tico
- âœ… Timeout configurable

Â¿Quieres que continÃºe con el cÃ³digo mejorado? ğŸš€

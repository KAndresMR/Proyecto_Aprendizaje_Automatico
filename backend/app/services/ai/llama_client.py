import json
import logging
import re
import time
from typing import Dict, Optional, List
from datetime import datetime

try:
    from langchain_ollama import OllamaLLM
    from langchain_core.prompts import PromptTemplate
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    logging.warning("‚ö†Ô∏è langchain-ollama no instalado. Usa: pip install langchain-ollama")

import requests

logger = logging.getLogger(__name__)


class LlamaClient:
    """
    Cliente para extraer informaci√≥n de productos usando Llama local.
    
    ENTRADA (extract method):
        ocr_text: str - Texto extra√≠do por OCR
        
    SALIDA (extract method):
        dict - JSON estructurado con datos del producto
        
    EJEMPLO:
        client = LlamaClient()
        result = client.extract("LECHE GLORIA 1L...")
        # ‚Üí {"name": "Leche Gloria", "brand": "Gloria", ...}
    """
    
    # Modelos disponibles (ordenados por calidad)
    AVAILABLE_MODELS = [
        "llama3.2:3b",    # Recomendado (balance)
        "llama3.2:1b",    # R√°pido (menos preciso)
        "llama3.1:8b",    # M√°s preciso (m√°s lento)
        "llama3.2",       # Alias de 3b
    ]
    
    def __init__(
        self,
        model: str = "llama3.2:latest",
        base_url: str = "http://localhost:11434",
        timeout: int = 60,
        max_retries: int = 2
    ):
        """
        Inicializa el cliente Llama.
        
        Args:
            model: Modelo a usar (ej: "llama3.2:latest")
            base_url: URL del servidor Ollama
            timeout: Tiempo m√°ximo de espera (segundos)
            max_retries: Reintentos en caso de error
        """
        self.model = model
        self.base_url = base_url
        self.timeout = timeout
        self.max_retries = max_retries
        self.llm = None
        self.prompt = None
        
        # Verificar que LangChain est√° disponible
        if not LANGCHAIN_AVAILABLE:
            logger.error("‚ùå LangChain no est√° instalado")
            logger.info("üí° Instala con: pip install langchain-ollama")
            return
        
        # Verificar que Ollama est√° corriendo
        if not self._check_ollama_server():
            logger.error(f"‚ùå Ollama no est√° corriendo en {base_url}")
            logger.info("üí° Inicia Ollama con: ollama serve")
            return
        
        # Verificar que el modelo est√° descargado
        if not self._check_model_exists():
            logger.error(f"‚ùå Modelo '{model}' no encontrado")
            logger.info(f"üí° Descarga con: ollama pull {model}")
            self._suggest_alternative_models()
            return
        
        # Inicializar LLM
        try:
            self.llm = OllamaLLM(
                model=model,
                base_url=base_url,
                temperature=0,  # Determinista
                timeout=timeout,
                num_predict=1024,  # Tokens m√°ximos de respuesta
            )
            
            # Crear prompt template
            self.prompt = self._create_prompt_template()
            
            logger.info(f"‚úÖ Llama inicializado: {model}")
            logger.info(f"üåê Servidor: {base_url}")
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando Llama: {e}")
            self.llm = None
    
    def _check_ollama_server(self) -> bool:
        """Verifica que el servidor Ollama est√° corriendo"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False
    
    def _check_model_exists(self) -> bool:
        """Verifica que el modelo est√° descargado"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code != 200:
                return False
            
            models = response.json().get("models", [])
            model_names = [m["name"] for m in models]
            
            # Verificar modelo exacto o base
            model_base = self.model.split(":")[0]  # "llama3.2:latest" ‚Üí "llama3.2"
            
            return any(
                self.model in name or model_base in name
                for name in model_names
            )
            
        except Exception:
            return False
    
    def _suggest_alternative_models(self):
        """Sugiere modelos alternativos disponibles"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                if models:
                    logger.info("üì¶ Modelos disponibles:")
                    for m in models:
                        logger.info(f"   - {m['name']}")
        except Exception:
            pass
    
    def _create_prompt_template(self) -> PromptTemplate:
        """Crea el template del prompt para extracci√≥n"""
        return PromptTemplate(
            input_variables=["ocr_text"],
            template="""Eres un experto en an√°lisis de productos de consumo.

Extrae informaci√≥n estructurada del siguiente texto OCR.
El texto puede contener errores de OCR y estar desordenado.

TEXTO OCR:
{ocr_text}

INSTRUCCIONES:
1. Corrige errores de OCR
2. Extrae toda la informaci√≥n posible
3. Si no encuentras un valor, usa null
4. Responde SOLO con JSON v√°lido (sin markdown, sin explicaciones)

ESTRUCTURA JSON REQUERIDA:
{{
  "name": null,
  "brand": null,
  "presentation": null,
  "size": null,
  "barcode": null,
  "batch": null,
  "expiry_date": null,
  "price": null,
  "category": null,
  "nutritional_info": {{
    "calories": null,
    "protein": null,
    "carbs": null,
    "fat": null,
    "sodium": null
  }}
}}

JSON:"""
        )
    
    def extract(self, ocr_text: str) -> Dict:
        """
        Extrae informaci√≥n del producto desde texto OCR.
        
        FLUJO:
        1. Valida que Llama est√° disponible
        2. Crea la cadena (Prompt + LLM)
        3. Ejecuta la inferencia
        4. Parsea y valida JSON
        5. Retorna resultado
        
        Args:
            ocr_text: Texto extra√≠do por OCR
            
        Returns:
            Dict con informaci√≥n del producto
            
        Raises:
            Exception: Si Llama no est√° disponible o falla la extracci√≥n
        """
        # Validar disponibilidad
        if not self.llm:
            raise Exception(
                "Llama no est√° disponible. "
                "Verifica que Ollama est√© corriendo y el modelo descargado."
            )
        
        # Validar entrada
        if not ocr_text or not ocr_text.strip():
            raise ValueError("OCR text est√° vac√≠o")
        
        logger.info(f"ü¶ô Extrayendo con Llama | Texto length: {len(ocr_text)}")
        
        # Intentar extracci√≥n con retries
        last_error = None
        
        for attempt in range(1, self.max_retries + 1):
            try:
                start_time = time.time()
                
                # Crear cadena (Prompt ‚Üí LLM)
                chain = self.prompt | self.llm
                # Ejecutar inferencia
                logger.info(f"üîÑ Intento {attempt}/{self.max_retries}")
                response = chain.invoke({"ocr_text": ocr_text})
                if not isinstance(response, str):
                    response = str(response)
                
                elapsed = time.time() - start_time
                logger.info(f"‚è±Ô∏è  Llama respondi√≥ en {elapsed:.2f}s")
                
                # Parsear JSON de la respuesta
                result = self._extract_json_from_response(response)
                
                # Validar estructura
                self._validate_result(result)
                
                logger.info(f"‚úÖ Extracci√≥n exitosa con Llama")
                return result
                
            except Exception as e:
                last_error = e
                logger.warning(f"‚ö†Ô∏è  Intento {attempt} fall√≥: {e}")
                
                if attempt < self.max_retries:
                    wait_time = attempt * 2  # Backoff exponencial
                    logger.info(f"‚è≥ Reintentando en {wait_time}s...")
                    time.sleep(wait_time)
        
        # Si llegamos aqu√≠, todos los intentos fallaron
        logger.error(f"‚ùå Llama fall√≥ despu√©s de {self.max_retries} intentos")
        raise last_error
    
    def _extract_json_from_response(self, response: str) -> Dict:
        """
        Extrae y parsea JSON de la respuesta de Llama.
        
        PROBLEMA: Llama a veces responde con:
        ```json
        {"name": "..."}
        ```
        
        SOLUCI√ìN: Limpiar markdown y extraer JSON puro
        
        Args:
            response: Respuesta cruda de Llama
            
        Returns:
            Dict parseado
            
        Raises:
            json.JSONDecodeError: Si no se puede parsear
        """
        original_response = response
        response = response.strip()
        
        # PASO 1: Limpiar markdown
        # Remover ```json ... ```
        if response.startswith("```"):
            lines = response.split("\n")
            # Remover primera l√≠nea (```json)
            if lines[0].strip().startswith("```"):
                lines = lines[1:]
            # Remover √∫ltima l√≠nea (```)
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            response = "\n".join(lines)
        
        # PASO 2: Buscar JSON en la respuesta
        # Patr√≥n: { ... } (incluso con saltos de l√≠nea)
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        
        if json_match:
            response = json_match.group()
        
        # PASO 3: Parsear JSON
        try:
            return json.loads(response)
        except json.JSONDecodeError as e:
            logger.error(f"Error parseando JSON de Llama:")
            logger.error(f"Respuesta original: {original_response[:500]}")
            logger.error(f"Despu√©s de limpieza: {response[:500]}")
            raise ValueError(f"Llama no devolvi√≥ JSON v√°lido: {str(e)}")
    
    def _validate_result(self, result: Dict):
        """
        Valida que el resultado tenga la estructura correcta.
        
        Args:
            result: Dict a validar
            
        Raises:
            ValueError: Si falta alguna clave requerida
        """
        required_keys = [
            "name", "brand", "presentation", "size", "barcode",
            "batch", "expiry_date", "price", "category", "nutritional_info"
        ]
        
        missing_keys = [key for key in required_keys if key not in result]
        
        if missing_keys:
            raise ValueError(
                f"Estructura incompleta de Llama. "
                f"Claves faltantes: {missing_keys}"
            )
        
        # Validar nutritional_info
        if not isinstance(result["nutritional_info"], dict):
            raise ValueError("nutritional_info debe ser un diccionario")
    
    def get_model_info(self) -> Dict:
        """
        Obtiene informaci√≥n del modelo activo.
        
        Returns:
            Dict con metadata del modelo
        """
        if not self.llm:
            return {"error": "Llama no disponible"}
        
        try:
            response = requests.post(
                f"{self.base_url}/api/show",
                json={"name": self.model},
                timeout=5
            )
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "model": self.model,
                    "size": data.get("size", "unknown"),
                    "modified": data.get("modified_at", "unknown"),
                    "available": True
                }
        except Exception as e:
            logger.error(f"Error obteniendo info del modelo: {e}")
        
        return {
            "model": self.model,
            "available": self.llm is not None
        }
    
    def is_available(self) -> bool:
        """Verifica si Llama est√° listo para usar"""
        return self.llm is not None


# ========================================
# INSTANCIA GLOBAL
# ========================================

try:
    llama_client = LlamaClient(
        model="llama3.2:latest",  # Cambia seg√∫n tu modelo
        timeout=60
    )
    
    if llama_client.is_available():
        logger.info("‚úÖ LlamaClient global inicializado")
        info = llama_client.get_model_info()
        logger.info(f"üì¶ Modelo: {info.get('model')}")
    else:
        logger.warning("‚ö†Ô∏è LlamaClient no disponible")
        llama_client = None
        
except Exception as e:
    logger.error(f"‚ùå Error creando LlamaClient global: {e}")
    llama_client = None


# ========================================
# UTILIDADES
# ========================================

def test_llama_connection():
    """
    Prueba la conexi√≥n con Ollama.
    √ötil para debugging.
    """
    print("\nüîç Probando conexi√≥n con Llama...\n")
    
    # 1. Verificar Ollama
    print("1Ô∏è‚É£ Verificando servidor Ollama...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("   ‚úÖ Ollama est√° corriendo")
            
            models = response.json().get("models", [])
            if models:
                print(f"   üì¶ Modelos disponibles ({len(models)}):")
                for m in models:
                    print(f"      - {m['name']}")
            else:
                print("   ‚ö†Ô∏è  No hay modelos descargados")
                print("   üí° Descarga uno con: ollama pull llama3.2:latest")
        else:
            print("   ‚ùå Ollama respondi√≥ con error")
    except requests.exceptions.RequestException:
        print("   ‚ùå Ollama no est√° corriendo")
        print("   üí° Inicia con: ollama serve")
        return
    
    # 2. Verificar LangChain
    print("\n2Ô∏è‚É£ Verificando LangChain...")
    if LANGCHAIN_AVAILABLE:
        print("   ‚úÖ langchain-ollama instalado")
    else:
        print("   ‚ùå langchain-ollama no instalado")
        print("   üí° Instala con: pip install langchain-ollama")
        return
    
    # 3. Probar LlamaClient
    print("\n3Ô∏è‚É£ Probando LlamaClient...")
    try:
        client = LlamaClient()
        if client.is_available():
            print("   ‚úÖ LlamaClient inicializado")
            
            # Prueba simple
            print("\n4Ô∏è‚É£ Probando extracci√≥n...")
            test_text = "LECHE GLORIA\nENTERA\n1000 ml\nS/ 5.50"
            
            result = client.extract(test_text)
            
            print("   ‚úÖ Extracci√≥n exitosa:")
            print(f"      - Nombre: {result.get('name')}")
            print(f"      - Marca: {result.get('brand')}")
            print(f"      - Tama√±o: {result.get('size')}")
            print(f"      - Precio: {result.get('price')}")
        else:
            print("   ‚ùå LlamaClient no disponible")
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
    
    print("\n‚ú® Prueba completada\n")


if __name__ == "__main__":
    # Ejecutar prueba si se corre directamente
    test_llama_connection()
